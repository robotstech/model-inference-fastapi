# tensorflow-model-inference-fastapi
Running inference on tensorflow model over http using fastapi
